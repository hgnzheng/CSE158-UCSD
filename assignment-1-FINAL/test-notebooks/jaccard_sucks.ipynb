{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Would Play prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions and data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        g = d['gameID']\n",
    "        yield u,g,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allHours = []\n",
    "for l in readJSON(\"train.json.gz\"):\n",
    "    allHours.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursTrain = allHours[:165000]\n",
    "hoursValid = allHours[165000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoursPerUser = defaultdict(list)\n",
    "hoursPerItem = defaultdict(list)\n",
    "for u,g,d in hoursTrain:\n",
    "    r = d['hours_transformed']\n",
    "    hoursPerUser[u].append((g, r))\n",
    "    hoursPerItem[g].append((u, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(list)\n",
    "for u,g,d in hoursTrain:\n",
    "    itemsPerUser[u].append(g)\n",
    "    usersPerItem[g].append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIDs, itemIDs = {}, {}\n",
    "interactions = []\n",
    "\n",
    "for u,g,d in allHours:\n",
    "    if not u in userIDs: userIDs[u] = len(userIDs)\n",
    "    if not g in itemIDs: itemIDs[g] = len(itemIDs)\n",
    "    label = 1 # they are positive labels we presume --> might improve by sentiment analysis on text data\n",
    "    interactions.append((u,g,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactionsTrain = interactions[:165000]\n",
    "interactionsTest = interactions[165000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a negative set\n",
    "userSet = set()\n",
    "gameSet = set()\n",
    "playedSet = set()\n",
    "\n",
    "for u,g,d in allHours:\n",
    "    userSet.add(u)\n",
    "    gameSet.add(g)\n",
    "    playedSet.add((u, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6710, 2437)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(userSet), len(gameSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lUserSet = list(userSet)\n",
    "lGameSet = list(gameSet)\n",
    "userPassed = set()\n",
    "\n",
    "notPlayed = set()\n",
    "for u,g,d in hoursValid:\n",
    "    g = random.choice(lGameSet)\n",
    "    while (u,g) in playedSet or (u,g) in notPlayed:\n",
    "        g = random.choice(lGameSet)\n",
    "    notPlayed.add((u,g))\n",
    "\n",
    "playedValid = set()\n",
    "for u,g,r in hoursValid:\n",
    "    playedValid.add((u,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CosineSet(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = math.sqrt(len(s1)) * math.sqrt(len(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(max_thre, hours):\n",
    "    correct = 0\n",
    "    p0, p1 = 0,0\n",
    "    for (label, sample) in [(1, playedValid), (0, notPlayed)]:\n",
    "        for (u,g) in sample:\n",
    "            maxSim = 0\n",
    "            users = set(hoursPerItem[g])\n",
    "            for g2,_ in hoursPerItem[u]:\n",
    "                sim = CosineSet(users, set(hoursPerItem[g2]))\n",
    "                if sim > maxSim:\n",
    "                    maxSim = sim\n",
    "            pred = 0\n",
    "            if maxSim > max_thre or len(hoursPerItem[g]) > hours:\n",
    "                pred = 1\n",
    "                p1 += 1\n",
    "            else:\n",
    "                p0 += 1\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "    acc = correct / (len(playedValid) + len(notPlayed))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(50,100) -> (50,80) -> (71,72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_tune_param_hour(max_thre=0.0025):  \n",
    "    max_acc = 0\n",
    "    bestHour = 50\n",
    "    counter = 0\n",
    "    for hour in np.arange(70, 72, 0.01):\n",
    "        acc = compute_acc(max_thre, hour)\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            bestHour = hour\n",
    "        if counter % 20 == 19:\n",
    "            print(f\"bestHour at hour = {hour} is {bestHour}\")\n",
    "        counter += 1\n",
    "    return bestHour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestHour at hour = 70.1900000000001 is 70.0\n",
      "bestHour at hour = 70.3900000000002 is 70.0\n",
      "bestHour at hour = 70.5900000000003 is 70.0\n",
      "bestHour at hour = 70.7900000000004 is 70.0\n",
      "bestHour at hour = 70.9900000000005 is 70.0\n",
      "bestHour at hour = 71.19000000000061 is 71.00000000000051\n",
      "bestHour at hour = 71.39000000000071 is 71.00000000000051\n",
      "bestHour at hour = 71.59000000000081 is 71.00000000000051\n",
      "bestHour at hour = 71.79000000000092 is 71.00000000000051\n",
      "bestHour at hour = 71.99000000000102 is 71.00000000000051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71.00000000000051"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_tune_param_hour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_tune_param_threshold(hour=71.00000000000051):  \n",
    "    max_acc = 0\n",
    "    bestThreshold = 0\n",
    "    counter = 0\n",
    "    for threshold in np.arange(0.0025,0.5,0.0025):\n",
    "        acc = compute_acc(threshold, hour)\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            bestThreshold = threshold\n",
    "        if counter % 20 == 19:\n",
    "            print(f\"bestThreshold at threshold = {threshold} is {bestThreshold}\")\n",
    "        counter += 1\n",
    "    return bestThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestThreshold at threshold = 0.05 is 0.0025\n",
      "bestThreshold at threshold = 0.1 is 0.0025\n",
      "bestThreshold at threshold = 0.15 is 0.0025\n",
      "bestThreshold at threshold = 0.2 is 0.0025\n",
      "bestThreshold at threshold = 0.25 is 0.0025\n",
      "bestThreshold at threshold = 0.3 is 0.0025\n",
      "bestThreshold at threshold = 0.35000000000000003 is 0.0025\n",
      "bestThreshold at threshold = 0.4 is 0.0025\n",
      "bestThreshold at threshold = 0.45 is 0.0025\n"
     ]
    }
   ],
   "source": [
    "bestThreshold = cos_tune_param_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7052705270527053"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = compute_acc(max_thre=0.0025, hours=71.00000000000051)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1,s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(max_thre, hours, sim_func=Jaccard):\n",
    "    correct = 0\n",
    "    p0, p1 = 0,0\n",
    "    for (label, sample) in [(1, playedValid), (0, notPlayed)]:\n",
    "        for (u,g) in sample:\n",
    "            maxSim = 0\n",
    "            users = set(hoursPerItem[g])\n",
    "            for g2,_ in hoursPerItem[u]:\n",
    "                sim = sim_func(users, set(hoursPerItem[g2]))\n",
    "                if sim > maxSim:\n",
    "                    maxSim = sim\n",
    "            pred = 0\n",
    "            if maxSim > max_thre or len(hoursPerItem[g]) > hours:\n",
    "                pred = 1\n",
    "                p1 += 1\n",
    "            else:\n",
    "                p0 += 1\n",
    "            if pred == label:\n",
    "                correct += 1\n",
    "    acc = correct / (len(playedValid) + len(notPlayed))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_tune_param_hour(max_thre=0.0025):  \n",
    "    max_acc = 0\n",
    "    bestHour = 0\n",
    "    counter = 0\n",
    "    for hour in np.arange(70, 72, 0.01):\n",
    "        acc = compute_acc(max_thre, hour)\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            bestHour = hour\n",
    "        if counter % 20 == 19:\n",
    "            print(f\"bestHour at hour = {hour} is {bestHour}\")\n",
    "        counter += 1\n",
    "    return bestHour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestHour at hour = 70.1900000000001 is 70.0\n",
      "bestHour at hour = 70.3900000000002 is 70.0\n",
      "bestHour at hour = 70.5900000000003 is 70.0\n",
      "bestHour at hour = 70.7900000000004 is 70.0\n",
      "bestHour at hour = 70.9900000000005 is 70.0\n",
      "bestHour at hour = 71.19000000000061 is 71.00000000000051\n",
      "bestHour at hour = 71.39000000000071 is 71.00000000000051\n",
      "bestHour at hour = 71.59000000000081 is 71.00000000000051\n",
      "bestHour at hour = 71.79000000000092 is 71.00000000000051\n",
      "bestHour at hour = 71.99000000000102 is 71.00000000000051\n"
     ]
    }
   ],
   "source": [
    "bestHoursJaccard = jaccard_tune_param_hour()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_tune_param_threshold(hour=71.00000000000051):  \n",
    "    max_acc = 0\n",
    "    bestThreshold = 0\n",
    "    counter = 0\n",
    "    for threshold in np.arange(0,0.2,0.001):\n",
    "        acc = compute_acc(threshold, hour)\n",
    "        if acc > max_acc:\n",
    "            max_acc = acc\n",
    "            bestThreshold = threshold\n",
    "        if counter % 20 == 19:\n",
    "            print(f\"bestThreshold at threshold = {threshold} is {bestThreshold}\")\n",
    "        counter += 1\n",
    "    return bestThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestThreshold at threshold = 0.019 is 0.0\n",
      "bestThreshold at threshold = 0.039 is 0.0\n",
      "bestThreshold at threshold = 0.059000000000000004 is 0.0\n",
      "bestThreshold at threshold = 0.079 is 0.0\n",
      "bestThreshold at threshold = 0.099 is 0.0\n",
      "bestThreshold at threshold = 0.11900000000000001 is 0.0\n",
      "bestThreshold at threshold = 0.139 is 0.0\n",
      "bestThreshold at threshold = 0.159 is 0.0\n",
      "bestThreshold at threshold = 0.179 is 0.0\n",
      "bestThreshold at threshold = 0.199 is 0.0\n"
     ]
    }
   ],
   "source": [
    "bestThresholdJaccard = cos_tune_param_threshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7052705270527053"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accJaccard = compute_acc(hours=bestHoursJaccard, max_thre=bestThresholdJaccard)\n",
    "accJaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Played.csv\", 'w')\n",
    "for l in open(\"pairs_Played.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "    maxSim = 0\n",
    "    users = set(hoursPerItem[g])\n",
    "    for g2,_ in hoursPerUser[u]:\n",
    "        sim = Jaccard(users, hoursPerItem[g2])\n",
    "        if sim > maxSim:\n",
    "            maxSim = sim\n",
    "    pred = 0\n",
    "    if maxSim > 0.025 or len(hoursPerItem[g]) > 71.00000000000051:\n",
    "        pred = 1\n",
    "    _ = predictions.write(u + ',' + g + ',' + str(pred) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "p0, p1 = 0,0\n",
    "for (label, sample) in [(1, playedValid), (0, notPlayed)]:\n",
    "    for (u,g) in sample:\n",
    "        maxSim = 0\n",
    "        users = set(hoursPerItem[g])\n",
    "        for g2,_ in hoursPerItem[u]:\n",
    "            sim = CosineSet(users, set(hoursPerItem[g2]))\n",
    "            if sim > maxSim:\n",
    "                maxSim = sim\n",
    "        pred = 0\n",
    "        if maxSim > 0.0025 or len(hoursPerItem[g]) > 60:\n",
    "            pred = 1\n",
    "            p1 += 1\n",
    "        else:\n",
    "            p0 += 1\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "acc = correct / (len(playedValid) + len(notPlayed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7031203120312031"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / (len(playedValid) + len(notPlayed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay... they both suck. Let's get rid of these models. Or we can ensemble them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, let's try Bayesian Personalized Ranking. Firstly, we need a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174999"
      ]
     },
     "execution_count": 656,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsPerUser = defaultdict(set)\n",
    "for u,i,_ in interactionsTrain:\n",
    "    itemsPerUser[u].add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(itemIDs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Experiment with learning rate -> 0.1 is the best\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent does not work well on this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam Learning Rate | AUC Score          | Leaderboard score\n",
    "\n",
    "0.1                | 0.7974952114458922 | 0.7292\n",
    "\n",
    "0.05               | 0.7865723946101689 | 0.7163\n",
    "\n",
    "0.15               | 0.7879794800046623 | 0.7249\n",
    "\n",
    "0.08               | 0.791222955079197  | 0.7244\n",
    "\n",
    "0.12               | 0.7902243541697886 | 0.7248\n",
    "\n",
    "0.11               | 0.7847410849614408 | 0.7255\n",
    "\n",
    "Conclusion: 0.1 is the best learning rate we have.\n",
    "\n",
    "0.7948501807158672 -> 200 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Experiment with learning rate -> 0.1 is the best\n",
    "optimizer = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "class BPRbatch(tf.keras.Model):\n",
    "    def __init__(self, K, lamb):\n",
    "        super(BPRbatch, self).__init__()\n",
    "        # Initialize variables\n",
    "        self.betaI = tf.Variable(tf.random.normal([len(gameSet)], stddev=0.001))\n",
    "        self.gammaU = tf.Variable(tf.random.normal([len(userSet), K], stddev=0.001))\n",
    "        self.gammaI = tf.Variable(tf.random.normal([len(gameSet), K], stddev=0.001))\n",
    "        # Regularization coefficient\n",
    "        self.lamb = lamb\n",
    "    \n",
    "    # Prediction for a single instance\n",
    "    def predict(self, u, i):\n",
    "        p = self.betaI[i] + tf.tensordot(self.gammaU[u], self.gammaI[i], 1)\n",
    "        return p\n",
    "    \n",
    "    # Regularizer\n",
    "    ### POTENTIALLY SEPARATE LAMBDA FOR BETA AND GAMMA ###\n",
    "    def reg(self):\n",
    "        return self.lamb * (tf.nn.l2_loss(self.betaI) +\\\n",
    "                            tf.nn.l2_loss(self.gammaU) +\\\n",
    "                            tf.nn.l2_loss(self.gammaI))\n",
    "    \n",
    "    def score(self, sampleU, sampleI):\n",
    "        u = tf.convert_to_tensor(sampleU, dtype=tf.int32)\n",
    "        i = tf.convert_to_tensor(sampleI, dtype=tf.int32)\n",
    "        beta_i = tf.nn.embedding_lookup(self.betaI, i)\n",
    "        gamma_u = tf.nn.embedding_lookup(self.gammaU, u)\n",
    "        gamma_i = tf.nn.embedding_lookup(self.gammaI, i)\n",
    "        x_ui = beta_i + tf.reduce_sum(tf.multiply(gamma_u, gamma_i), 1)\n",
    "        return x_ui\n",
    "    \n",
    "    def call(self, sampleU, sampleI, sampleJ):\n",
    "        x_ui = self.score(sampleU, sampleI)\n",
    "        x_uj = self.score(sampleU, sampleJ)\n",
    "        return -tf.reduce_mean(tf.math.log(tf.math.sigmoid(x_ui - x_uj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a single lambda\n",
    "\n",
    "Lambda   | Objective   | AUC                | Leaderboard\n",
    "\n",
    "0.00001  | 0.44241133  | 0.7886660337447313 | N/A\n",
    "\n",
    "0.0001   | 0.54610515  | 0.7520641582971066 | 0.6787\n",
    "\n",
    "0.001    | 0.6417916   | 0.7452726561068631 | 0.5544\n",
    "\n",
    "With two lambdas\n",
    "\n",
    "0.00001, 0.0001 | 0.7560826296240774 | 0.6977\n",
    "\n",
    "0.0001, 0.00001 | 0.7919284964772884 | 0.7207*\n",
    "\n",
    "0.00001, 0.000001 | 0.7764748746648007 | 0.7095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr=0.1, K=5, lambda=0.00001, threshold=0.59, leader=0.7285\n",
    "\n",
    "K=6 AUC: 0.7933193424274209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelBPR = BPRbatch(6, 0.00001) # need to experiment lambda too\n",
    "\n",
    "def trainingStepBPR(model, interactions):\n",
    "    Nsamples = 50000\n",
    "    with tf.GradientTape() as tape:\n",
    "        sampleU, sampleI, sampleJ = [],[],[]\n",
    "        for _ in range(Nsamples):\n",
    "            u,i,_ = random.choice(interactions) # positive sample\n",
    "            j = random.choice(items) # negative sample\n",
    "            while j in itemsPerUser[u]:\n",
    "                j = random.choice(items)\n",
    "            sampleU.append(userIDs[u])\n",
    "            sampleI.append(itemIDs[i])\n",
    "            sampleJ.append(itemIDs[j])\n",
    "        \n",
    "        loss = model(sampleU, sampleI, sampleJ)\n",
    "        loss += model.reg()\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients((grad, var) for \n",
    "                              (grad, var) in zip(gradients, model.trainable_variables)\n",
    "                              if grad is not None)\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "200 epochs does not improve much compared with 100 epochs. But we got almost to the lowest objective value at 120 epochs. To avoid underfitting though, we still use 200 epochs when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, objective = 0.4877563\n",
      "Iteration 40, objective = 0.45658654\n",
      "Iteration 60, objective = 0.4510499\n",
      "Iteration 80, objective = 0.44498116\n",
      "Iteration 100, objective = 0.4404919\n",
      "Iteration 120, objective = 0.43773398\n",
      "Iteration 140, objective = 0.43912643\n",
      "Iteration 160, objective = 0.43745488\n",
      "Iteration 180, objective = 0.43903378\n",
      "Iteration 200, objective = 0.43722948\n"
     ]
    }
   ],
   "source": [
    "# Run 200 batches of gradient descent, the code below has been run for several times\n",
    "for i in range(200):\n",
    "    obj = trainingStepBPR(modelBPR, interactionsTrain)\n",
    "    if (i % 20 == 19): print(\"Iteration \" + str(i+1) + \", objective = \" + str(obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6710, 2437, 174999)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(userSet), len(gameSet), len(playedSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactionsTestPerUser = defaultdict(set)\n",
    "itemSet = set()\n",
    "for u,i,_ in interactionsTest:\n",
    "    interactionsTestPerUser[u].add(i)\n",
    "    itemSet.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUCu(u, N): # N samples per user\n",
    "    win = 0\n",
    "    if N > len(interactionsTestPerUser[u]):\n",
    "        N = len(interactionsTestPerUser[u])\n",
    "    positive = random.sample(interactionsTestPerUser[u],N)\n",
    "    negative = random.sample(gameSet.difference(interactionsTestPerUser[u]),N)\n",
    "    for i,j in zip(positive,negative):\n",
    "        si = modelBPR.predict(userIDs[u], itemIDs[i]).numpy()\n",
    "        sj = modelBPR.predict(userIDs[u], itemIDs[j]).numpy()\n",
    "        if si > sj:\n",
    "            win += 1\n",
    "    return win/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC():\n",
    "    av = []\n",
    "    for u in interactionsTestPerUser:\n",
    "        av.append(AUCu(u, 10))\n",
    "    return sum(av) / len(av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8f/_6h5xprs32dfmm3jfs0g7k880000gn/T/ipykernel_38285/3135776419.py:5: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  positive = random.sample(interactionsTestPerUser[u],N)\n",
      "/var/folders/8f/_6h5xprs32dfmm3jfs0g7k880000gn/T/ipykernel_38285/3135776419.py:6: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  negative = random.sample(gameSet.difference(interactionsTestPerUser[u]),N)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7933193424274209"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for (label, sample) in [(1, playedValid), (0, notPlayed)]:\n",
    "    for (u,g) in sample:\n",
    "        score = modelBPR.predict(userIDs[u], itemIDs[g]).numpy()\n",
    "        scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.sort()\n",
    "scores.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.56898946, 0.56875527, 0.00036346912)"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[9999], scores[10000], scores[9999] - scores[10001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55580540174"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5561886 - 0.00038319826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.556"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "p0, p1 = 0,0\n",
    "scores = []\n",
    "for (label, sample) in [(1, playedValid), (0, notPlayed)]:\n",
    "    for (u,g) in sample:\n",
    "        score = modelBPR.predict(userIDs[u], itemIDs[g]).numpy()\n",
    "        scores.append(score)\n",
    "        \n",
    "        maxSim = 0\n",
    "        users = set(hoursPerItem[g])\n",
    "        for g2,_ in hoursPerItem[u]:\n",
    "            sim = CosineSet(users, set(hoursPerItem[g2]))\n",
    "            if sim > maxSim:\n",
    "                maxSim = sim\n",
    "        pred = 0\n",
    "        if maxSim > 0.0025 or len(hoursPerItem[g]) > 60:\n",
    "            pred = 1\n",
    "            p1 += 1\n",
    "        else:\n",
    "            p0 += 1\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "acc = correct / (len(playedValid) + len(notPlayed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [modelBPR.predict(userIDs[u], itemIDs[i]).numpy() for u,i,l in interactionsTrain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8930664"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.sort()\n",
    "scores[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165000, 1.2336296, -1.0530381)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores), max(scores), min(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33681310097428785"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalAverage = sum(scores) / len(scores)\n",
    "globalAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate negative labels\n",
    "notPlayed = set()\n",
    "for u,g,d in hoursValid:\n",
    "    g = random.choice(lGameSet)\n",
    "    while (u,g) in playedSet or (u,g) in notPlayed:\n",
    "        g = random.choice(lGameSet)\n",
    "    notPlayed.add((u,g, 1))\n",
    "\n",
    "playedValid = set()\n",
    "for u,g,r in hoursValid:\n",
    "    playedValid.add((u,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interactionsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemScorePerUser = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u,i,l in interactionsTrain:\n",
    "    score = modelBPR.predict(userIDs[u], itemIDs[i]).numpy()\n",
    "    itemScorePerUser[u].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in itemScorePerUser:\n",
    "    itemScorePerUser[u].sort()\n",
    "    itemScorePerUser[u].reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(value) for key,value in itemsPerUser.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.59016393442623"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lengths) / len(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameCount = defaultdict(int)\n",
    "totalPlayed = 0\n",
    "\n",
    "for user,game,_ in readJSON(\"train.json.gz\"):\n",
    "    gameCount[game] += 1\n",
    "    totalPlayed += 1\n",
    "\n",
    "mostPopular = [(gameCount[x], x) for x in gameCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalPlayed/1.4844999999999906: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "notPlayed = set()\n",
    "for u,g,d in hoursValid:\n",
    "    g = random.choice(lGameSet)\n",
    "    while (u,g) in playedSet or (u,g) in notPlayed:\n",
    "        g = random.choice(lGameSet)\n",
    "    notPlayed.add((u,g))\n",
    "\n",
    "playedValid = set()\n",
    "for u,g,r in hoursValid:\n",
    "    playedValid.add((u,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_eval(threshold=0):\n",
    "    correct = 0\n",
    "    for label,sample in [(1, playedValid), (0, notPlayed)]:\n",
    "        for (u,b) in sample:\n",
    "            pred = 0\n",
    "            if b in return1:\n",
    "                pred = 1\n",
    "            if u in userIDs and g in itemIDs:\n",
    "                if modelBPR.predict(userIDs[u], itemIDs[g]).numpy() > threshold:\n",
    "                    pred = 1\n",
    "            if label == pred: correct += 1\n",
    "    return correct / (len(playedValid) + len(notPlayed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestAcc = 0\n",
    "# bestThreshold = None\n",
    "# for threshold in np.arange(-1, 1, 0.1):\n",
    "#     acc = pred_eval(threshold)\n",
    "#     if acc > bestAcc:\n",
    "#         bestAcc = acc\n",
    "#         bestThreshold = threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8999999999999995, 0.5343534353435343)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestThreshold, bestAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5006000600060005"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / (len(playedValid) + len(notPlayed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19998"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(playedValid) + len(notPlayed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "notPlayed = set()\n",
    "for u,g,d in hoursValid:\n",
    "    g = random.choice(lGameSet)\n",
    "    while (u,g) in playedSet or (u,g) in notPlayed:\n",
    "        g = random.choice(lGameSet)\n",
    "    notPlayed.add((u,g))\n",
    "\n",
    "playedValid = set()\n",
    "for u,g,r in hoursValid:\n",
    "    playedValid.add((u,g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7031203120312031"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correct / (len(playedValid) + len(notPlayed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=5 --> thre=0.556\n",
    "\n",
    "k=3 --> thre=0.53543\n",
    "\n",
    "k=2 --> thre=0.49670 | 0.722\n",
    "\n",
    "k=4 --> thre=0.5456 | 0.7278\n",
    "\n",
    "k=6 --> 0.7286"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Played.csv\", 'w')\n",
    "for l in open(\"pairs_Played.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,g = l.strip().split(',')\n",
    "    pred = 0\n",
    "    if (u,g,1) in interactionsTrain:\n",
    "        pred = 1\n",
    "    if u in userIDs and g in itemIDs:\n",
    "        if modelBPR.predict(userIDs[u], itemIDs[g]).numpy() > 0.5689:\n",
    "            pred = 1\n",
    "    else:\n",
    "        if g in return1:\n",
    "            pred = 1\n",
    "    _ = predictions.write(u + ',' + g + ',' + str(pred) + '\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPR\n",
    "* Learning rate for Adam. --> Done. 0.1 is the best one.\n",
    "* Try out SGD and fine-tune the parameter --> Sucks, gone\n",
    "* Number of Epoch to train the model. --> 200.\n",
    "* Fine-tune the lambda -- maybe split into two lambdas. --> Done. Stick with the original one.\n",
    "* Threshold for label prediction.\n",
    "* Fine-tune the K-value of the BPR.\n",
    "* Ensemble different model predict (incorporate jaccard maybe?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LFM\n",
    "\n",
    "* Optimizer choice (Adam or SGD).\n",
    "* Learning rate for the optimizer.\n",
    "* Introduce two gamma terms.\n",
    "* K value (less than 5).\n",
    "* Alpha (offset term).\n",
    "* Regularizer (introduce two regularizers instead of one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Hours prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse158",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
